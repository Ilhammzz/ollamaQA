{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ee9dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from ast import literal_eval\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas.dataset_schema import EvaluationDataset, EvaluationResult\n",
    "from src.grag import run_text_generation_workflow, evaluate_text_generation\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249845be",
   "metadata": {},
   "source": [
    "## **Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b8e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah soal: 100\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = os.path.join(\"results\", \"llm_text_generation\")\n",
    "DATASET_PATH = os.path.join(\"data\", \"Dataset Testing.xlsx\")\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "df = pd.read_excel(DATASET_PATH)\n",
    "dataset = []\n",
    "expected_tool_call_names = []\n",
    "generated_cypher_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row[\"is_valid\"]:\n",
    "        dataset.append(\n",
    "            {\n",
    "                \"user_input\": str(row[\"user_input\"]),\n",
    "                \"reference\": str(row[\"reference\"]),\n",
    "                \"retrieved_contexts\": literal_eval(row[\"reference_contexts_1\"]),\n",
    "                # Atau\n",
    "                # \"retrieved_contexts\": literal_eval(row[\"reference_contexts_1\"]),\n",
    "                # \"reference_contexts\": literal_eval(row[\"reference_contexts_1\"]),\n",
    "            }\n",
    "        )\n",
    "        # Khusus Bayu\n",
    "        expected_tool_call_names.append(str(row[\"reference_tool_call\"]))\n",
    "        generated_cypher_results.append(str(row[\"cypher_reference\"]))\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "print(f\"Jumlah soal: {len(evaluation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAUDE_LLM_MODEL_NAME = \"claude-3-5-haiku-20241022\"\n",
    "claude_llm = ChatAnthropic(\n",
    "    model_name=CLAUDE_LLM_MODEL_NAME,\n",
    "    max_tokens_to_sample=4096,\n",
    "    temperature=0.0,\n",
    "    timeout=None,\n",
    "    api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
    ")\n",
    "\n",
    "LLAMA_LLM_MODEL_NAME = \"llama3.1:8b-instruct-q4_K_M\"\n",
    "llama_llm = ChatOllama(\n",
    "    model=LLAMA_LLM_MODEL_NAME,\n",
    "    num_ctx=32768,\n",
    "    num_predict=4096,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Evaluator\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
    "embedding_evaluator = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "llm_evaluator = claude_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40f0bb",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment_dataset_or_result(\n",
    "    dataset: Union[EvaluationDataset, EvaluationResult],\n",
    "    experiment_name: str\n",
    ") -> None:\n",
    "    dataset.to_pandas().to_json(\n",
    "        os.path.join(OUTPUT_PATH, f\"{experiment_name}.json\"),\n",
    "        orient=\"records\",\n",
    "    )\n",
    "\n",
    "\n",
    "def run_test_case(test_case: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    llm_type = \"api\" if test_case[\"llm_model_name\"].starts_with(\"claude\") else \"local\"\n",
    "    experiment_name = f\"{llm_type}_{test_case[\"llm_model_name\"]}\"\n",
    "\n",
    "    evaluation_dataset_completed = run_text_generation_workflow(\n",
    "        evaluation_dataset,\n",
    "        experiment_name,\n",
    "        expected_tool_call_names=expected_tool_call_names,\n",
    "        generated_cypher_results=generated_cypher_results,\n",
    "        llm=test_case[\"llm_model\"],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Checkpoint 1\n",
    "    save_experiment_dataset_or_result(\n",
    "        evaluation_dataset_completed,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "\n",
    "    # Break 60 seconds\n",
    "    time.sleep(60)\n",
    "\n",
    "    evaluation_result = evaluate_text_generation(\n",
    "        evaluation_dataset_completed,\n",
    "        llm_model=llm_evaluator,\n",
    "        embedding_model=embedding_evaluator,\n",
    "        experiment_name=experiment_name,\n",
    "    )\n",
    "\n",
    "    # Checkpoint 2\n",
    "    save_experiment_dataset_or_result(\n",
    "        evaluation_result,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"args\": {\"llm\": test_case[\"llm_model_name\"]},\n",
    "        \"evaluation_result\": evaluation_result,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        # Claude (API)\n",
    "        \"llm_model_name\": CLAUDE_LLM_MODEL_NAME,\n",
    "        \"llm_model\": claude_llm\n",
    "    },\n",
    "    {\n",
    "        # Llama (local)\n",
    "        \"llm_model_name\": LLAMA_LLM_MODEL_NAME,\n",
    "        \"llm_model\": llama_llm\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9bb1e",
   "metadata": {},
   "source": [
    "## **Test Case 1**\n",
    "\n",
    "- API: Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aef3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_1 = run_test_case(test_cases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(test_result_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c10131",
   "metadata": {},
   "source": [
    "## **Test Case 2**\n",
    "\n",
    "- Local: Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_2 = run_test_case(test_cases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e933c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(test_result_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
